#!/usr/bin/env python3
"""
æ–‡æ¡£è§£ææ¨¡å—æ¼”ç¤ºè„šæœ¬
å±•ç¤ºæ–°çš„æ–‡æ¡£è§£æåŠŸèƒ½
"""
import asyncio
import tempfile
import json
from pathlib import Path

from src.data.document_validator import document_validator
from src.data.parsers.text_parser import TextParser, MarkdownParser
from src.data.parsers.integrator import document_integrator
from src.data.parsers.cleaner import document_cleaner, document_formatter
from src.data.parsers.chunker import adaptive_chunker
from src.data.parsers.error_handler import global_error_handler
from src.utils.logger import get_logger

logger = get_logger("demo")


async def demo_text_parsing():
    """æ¼”ç¤ºæ–‡æœ¬è§£æåŠŸèƒ½"""
    print("\n=== æ–‡æœ¬è§£ææ¼”ç¤º ===")
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æœ¬æ–‡ä»¶
    content = """ä¼ä¸šç§æœ‰çŸ¥è¯†åº“ç³»ç»Ÿ

è¿™æ˜¯ä¸€ä¸ªåŸºäºRAGæŠ€æœ¯çš„ä¼ä¸šç§æœ‰çŸ¥è¯†åº“ç³»ç»Ÿã€‚

## ä¸»è¦åŠŸèƒ½

1. å¤šæ ¼å¼æ–‡æ¡£è§£æ
2. æ™ºèƒ½å†…å®¹æå–
3. å‘é‡åŒ–å­˜å‚¨
4. è¯­ä¹‰æ£€ç´¢

## æŠ€æœ¯ç‰¹ç‚¹

- æ”¯æŒ20+ç§æ–‡æ¡£æ ¼å¼
- å¤šæ¨¡æ€å†…å®¹å¤„ç†
- æ™ºèƒ½åˆ†å—ç­–ç•¥
- å¼ºå¤§çš„é”™è¯¯å¤„ç†

ç³»ç»Ÿé‡‡ç”¨åˆ†å±‚æ¶æ„ï¼Œç¡®ä¿é«˜æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚
"""
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False, encoding='utf-8') as f:
        f.write(content)
        temp_path = Path(f.name)
    
    try:
        # 1. æ–‡æ¡£éªŒè¯
        print("1. æ–‡æ¡£éªŒè¯...")
        doc_info = document_validator.validate_file(temp_path)
        print(f"   æ–‡æ¡£ç±»å‹: {doc_info.file_type.value}")
        print(f"   æ–‡ä»¶å¤§å°: {doc_info.file_size} å­—èŠ‚")
        print(f"   éªŒè¯ç»“æœ: {'é€šè¿‡' if doc_info.is_valid else 'å¤±è´¥'}")
        
        # 2. æ–‡æ¡£è§£æ
        print("\n2. æ–‡æ¡£è§£æ...")
        parser = MarkdownParser()
        parse_result = await parser.parse(temp_path, doc_info)
        print(f"   è§£æç»“æœ: {'æˆåŠŸ' if parse_result.success else 'å¤±è´¥'}")
        print(f"   æå–å…ƒç´ : {len(parse_result.elements)} ä¸ª")
        
        # æ˜¾ç¤ºè§£æçš„å…ƒç´ 
        for i, element in enumerate(parse_result.elements[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"   å…ƒç´  {i+1}: {element.element_type}")
            print(f"     å†…å®¹é¢„è§ˆ: {element.content[:50]}...")
        
        # 3. å†…å®¹æ•´åˆ
        print("\n3. å†…å®¹æ•´åˆ...")
        integrated_doc = await document_integrator.integrate_parse_results([parse_result], doc_info)
        print(f"   æ•´åˆå…ƒç´ : {len(integrated_doc.elements)} ä¸ª")
        print(f"   ç»“æ„ä¿¡æ¯: {len(integrated_doc.structure_map)} ä¸ªéƒ¨åˆ†")
        
        # 4. å†…å®¹æ¸…æ´—
        print("\n4. å†…å®¹æ¸…æ´—...")
        cleaned_doc = document_cleaner.clean_integrated_document(integrated_doc)
        print(f"   æ¸…æ´—åå…ƒç´ : {len(cleaned_doc.elements)} ä¸ª")
        
        # 5. æ™ºèƒ½åˆ†å—
        print("\n5. æ™ºèƒ½åˆ†å—...")
        documents = adaptive_chunker.chunk_integrated_document(cleaned_doc)
        print(f"   ç”Ÿæˆæ–‡æ¡£å—: {len(documents)} ä¸ª")
        
        # æ˜¾ç¤ºåˆ†å—ç»“æœ
        for i, doc in enumerate(documents[:2]):  # åªæ˜¾ç¤ºå‰2ä¸ª
            print(f"   å— {i+1}: {len(doc.page_content)} å­—ç¬¦")
            print(f"     å†…å®¹é¢„è§ˆ: {doc.page_content[:80]}...")
        
        # 6. æ ¼å¼åŒ–è¾“å‡º
        print("\n6. æ ¼å¼åŒ–è¾“å‡º...")
        rag_json = document_formatter.format_to_rag_json(cleaned_doc, temp_path)
        print(f"   JSONç»“æ„: {len(rag_json)} ä¸ªå­—æ®µ")
        print(f"   æ–‡æ¡£å…ƒæ•°æ®: {len(rag_json.get('document_metadata', {}))} ä¸ªå±æ€§")
        
        return True
        
    except Exception as e:
        logger.error(f"æ¼”ç¤ºå¤±è´¥: {e}")
        return False
        
    finally:
        temp_path.unlink()


async def demo_error_handling():
    """æ¼”ç¤ºé”™è¯¯å¤„ç†åŠŸèƒ½"""
    print("\n=== é”™è¯¯å¤„ç†æ¼”ç¤º ===")
    
    # 1. æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨é”™è¯¯
    print("1. æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨é”™è¯¯...")
    doc_info = document_validator.validate_file("nonexistent.txt")
    print(f"   éªŒè¯ç»“æœ: {'é€šè¿‡' if doc_info.is_valid else 'å¤±è´¥'}")
    print(f"   é”™è¯¯ä¿¡æ¯: {doc_info.error_message}")
    
    # 2. æµ‹è¯•ä¸æ”¯æŒçš„æ ¼å¼
    print("\n2. æµ‹è¯•ä¸æ”¯æŒçš„æ ¼å¼...")
    with tempfile.NamedTemporaryFile(suffix='.xyz', delete=False) as f:
        temp_path = Path(f.name)
    
    try:
        doc_info = document_validator.validate_file(temp_path)
        print(f"   éªŒè¯ç»“æœ: {'é€šè¿‡' if doc_info.is_valid else 'å¤±è´¥'}")
        print(f"   é”™è¯¯ä¿¡æ¯: {doc_info.error_message}")
        
    finally:
        temp_path.unlink()
    
    # 3. æ˜¾ç¤ºé”™è¯¯ç»Ÿè®¡
    print("\n3. é”™è¯¯ç»Ÿè®¡...")
    error_summary = global_error_handler.get_error_summary()
    print(f"   æ€»é”™è¯¯æ•°: {error_summary['total_errors']}")
    print(f"   é”™è¯¯ç±»å‹: {error_summary['error_types']}")
    print(f"   æ¢å¤æˆåŠŸç‡: {error_summary['recovery_success_rate']:.2%}")


def demo_supported_formats():
    """æ¼”ç¤ºæ”¯æŒçš„æ–‡æ¡£æ ¼å¼"""
    print("\n=== æ”¯æŒçš„æ–‡æ¡£æ ¼å¼ ===")
    
    from src.data.document_validator import DocumentType
    
    format_groups = {
        "Officeæ–‡æ¡£": [DocumentType.DOC, DocumentType.DOCX, DocumentType.PPT, DocumentType.PPTX, DocumentType.XLS, DocumentType.XLSX],
        "PDFæ–‡æ¡£": [DocumentType.PDF, DocumentType.PDF_SCANNED],
        "ç½‘é¡µå’Œæ ‡è®°": [DocumentType.HTML, DocumentType.MD, DocumentType.XML],
        "ç”µå­ä¹¦": [DocumentType.EPUB],
        "æ–‡æœ¬æ–‡æ¡£": [DocumentType.TXT],
        "é‚®ä»¶": [DocumentType.EML, DocumentType.MSG],
        "æ•°æ®æ–‡ä»¶": [DocumentType.CSV, DocumentType.JSON],
        "å›¾åƒæ–‡ä»¶": [DocumentType.JPG, DocumentType.JPEG, DocumentType.PNG, DocumentType.TIFF, DocumentType.TIF]
    }
    
    for group_name, formats in format_groups.items():
        print(f"\n{group_name}:")
        for fmt in formats:
            print(f"  - {fmt.value.upper()}: {fmt.name}")
    
    print(f"\næ€»è®¡æ”¯æŒ {sum(len(formats) for formats in format_groups.values())} ç§æ–‡æ¡£æ ¼å¼")


def demo_parsing_features():
    """æ¼”ç¤ºè§£æåŠŸèƒ½ç‰¹æ€§"""
    print("\n=== è§£æåŠŸèƒ½ç‰¹æ€§ ===")
    
    features = {
        "å¤šæ¨¡æ€è§£æ": [
            "æ–‡æœ¬å†…å®¹æå–",
            "å›¾åƒOCRè¯†åˆ«", 
            "è¡¨æ ¼ç»“æ„åŒ–æå–",
            "æ•°å­¦å…¬å¼è¯†åˆ«"
        ],
        "æ™ºèƒ½æ•´åˆ": [
            "ä¸Šä¸‹æ–‡å…³ç³»ä¿ç•™",
            "äº¤å‰å¼•ç”¨å»ºç«‹",
            "ç»“æ„ä¿¡æ¯æ˜ å°„",
            "å…ƒç´ ä½ç½®è¿½è¸ª"
        ],
        "å†…å®¹æ¸…æ´—": [
            "å™ªå£°è‡ªåŠ¨ç§»é™¤",
            "æ ¼å¼æ ‡å‡†åŒ–",
            "OCRé”™è¯¯ä¿®å¤",
            "é‡å¤å†…å®¹å»é™¤"
        ],
        "åˆ†å—ç­–ç•¥": [
            "ç»“æ„åŒ–åˆ†å—",
            "è¯­ä¹‰è¾¹ç•Œåˆ†å‰²",
            "é€’å½’å­—ç¬¦åˆ†å‰²",
            "è‡ªé€‚åº”å¤§å°è°ƒæ•´"
        ],
        "é”™è¯¯å¤„ç†": [
            "å¤šçº§é”™è¯¯åˆ†ç±»",
            "è‡ªåŠ¨æ¢å¤ç­–ç•¥",
            "è¯¦ç»†é”™è¯¯æ—¥å¿—",
            "å®¹é”™æœºåˆ¶ä¿éšœ"
        ]
    }
    
    for category, feature_list in features.items():
        print(f"\n{category}:")
        for feature in feature_list:
            print(f"  âœ“ {feature}")


async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ ä¼ä¸šç§æœ‰çŸ¥è¯†åº“ç³»ç»Ÿ - æ–‡æ¡£è§£ææ¨¡å—æ¼”ç¤º")
    print("=" * 60)
    
    # 1. æ˜¾ç¤ºæ”¯æŒçš„æ ¼å¼
    demo_supported_formats()
    
    # 2. æ˜¾ç¤ºè§£æåŠŸèƒ½ç‰¹æ€§
    demo_parsing_features()
    
    # 3. æ¼”ç¤ºæ–‡æœ¬è§£æ
    success = await demo_text_parsing()
    
    # 4. æ¼”ç¤ºé”™è¯¯å¤„ç†
    await demo_error_handling()
    
    # 5. æ€»ç»“
    print("\n=== æ¼”ç¤ºæ€»ç»“ ===")
    if success:
        print("âœ… æ–‡æ¡£è§£ææ¨¡å—æ¼”ç¤ºæˆåŠŸå®Œæˆï¼")
        print("\nä¸»è¦äº®ç‚¹:")
        print("  â€¢ æ”¯æŒ20+ç§æ–‡æ¡£æ ¼å¼")
        print("  â€¢ å¤šæ¨¡æ€å†…å®¹è§£æ")
        print("  â€¢ æ™ºèƒ½åˆ†å—ç­–ç•¥")
        print("  â€¢ å¼ºå¤§çš„é”™è¯¯å¤„ç†")
        print("  â€¢ RAGå‹å¥½çš„è¾“å‡ºæ ¼å¼")
    else:
        print("âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
    
    print("\nğŸ“š æ–‡æ¡£è§£ææ¨¡å—å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹å¤„ç†æ‚¨çš„æ–‡æ¡£ï¼")


if __name__ == "__main__":
    asyncio.run(main())
